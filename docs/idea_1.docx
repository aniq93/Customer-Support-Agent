# Production-Grade Agentic System: Complete Implementation Guide

**Document Version:** 1.0  
**Date:** August 25, 2025  
**Author:** Senior Technical Leadership  
**Project:** Intelligent Customer Support Agent System

---

## Executive Summary

This document provides a comprehensive roadmap for building a production-grade agentic system from conception to deployment. As your technical leader, I'll guide you through each phase with specific implementation details, technology choices, and best practices learned from scaling enterprise systems.

**Project Overview:**
- **System Name:** IntelliAgent Customer Support Platform
- **Timeline:** 9-12 months (MVP in 4 months)
- **Team Size:** 6-8 engineers
- **Budget Estimate:** $500K-$750K (first year)

---

## Phase 1: Foundation & Architecture (Weeks 1-4)

### 1.1 Technical Requirements Analysis

**Business Requirements:**
- Handle 10K+ customer interactions daily
- Support 99.9% uptime SLA
- Multi-language support (English, Spanish, French initially)
- Integration with existing CRM systems
- GDPR/CCPA compliance

**Non-Functional Requirements:**
- Response time: <200ms for API calls
- Scalability: Handle 10x traffic spikes
- Security: SOC 2 Type II compliance
- Monitoring: Full observability stack

### 1.2 Technology Stack Decision Matrix

| Component | Technology | Justification | Alternatives Considered |
|-----------|------------|---------------|------------------------|
| **Backend Framework** | FastAPI (Python) | High performance, async support, auto documentation | Node.js/Express, Go/Gin |
| **Database Primary** | PostgreSQL 15+ | ACID compliance, JSON support, mature ecosystem | MySQL, MongoDB |
| **Vector Database** | Pinecone | Managed service, excellent performance | Weaviate, Qdrant |
| **Cache Layer** | Redis Cluster | High availability, pub/sub capabilities | Memcached, Hazelcast |
| **Message Queue** | Apache Kafka | High throughput, event streaming | RabbitMQ, AWS SQS |
| **Frontend** | Next.js 14 | SSR, excellent DX, production ready | React SPA, Vue.js |
| **Container Platform** | Kubernetes | Industry standard, scaling capabilities | Docker Swarm, ECS |
| **Cloud Provider** | AWS | Comprehensive services, reliability | GCP, Azure |

### 1.3 System Architecture Design

```
┌─────────────────────────────────────────────────────────┐
│                    Load Balancer (ALB)                  │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────┴───────────────────────────────────┐
│                  API Gateway                            │
│               (Kong/AWS API Gateway)                    │
└─────┬──────────────────────────────────┬────────────────┘
      │                                  │
┌─────┴─────────┐                   ┌────┴──────────┐
│  Web Portal   │                   │  Agent Core   │
│  (Next.js)    │                   │  (FastAPI)    │
└───────────────┘                   └────┬──────────┘
                                         │
                    ┌────────────────────┼────────────────────┐
                    │                    │                    │
              ┌─────┴─────┐       ┌──────┴───────┐    ┌──────┴──────┐
              │ PostgreSQL│       │    Redis     │    │   Pinecone  │
              │ Cluster   │       │   Cluster    │    │  (Vectors)  │
              └───────────┘       └──────────────┘    └─────────────┘
                    │
              ┌─────┴─────┐
              │   Kafka   │
              │ Cluster   │
              └───────────┘
```

### 1.4 Development Environment Setup

**Step 1: Repository Structure**
```
intelligagent/
├── backend/
│   ├── app/
│   │   ├── agents/          # Agent logic
│   │   ├── api/             # API endpoints
│   │   ├── core/            # Configuration
│   │   ├── db/              # Database models
│   │   └── services/        # Business logic
│   ├── tests/
│   ├── docker/
│   └── requirements/
├── frontend/
│   ├── components/
│   ├── pages/
│   ├── services/
│   └── utils/
├── infrastructure/
│   ├── terraform/
│   ├── helm-charts/
│   └── monitoring/
└── docs/
```

**Step 2: Local Development Stack (Docker Compose)**
```yaml
version: '3.8'
services:
  api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/intelligagent
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: intelligagent
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

### 1.5 Team Structure & Responsibilities

**Core Team (Phase 1):**
- **Tech Lead** (you): Architecture decisions, code reviews
- **Backend Engineer**: API development, database design
- **Frontend Engineer**: React components, UI/UX
- **DevOps Engineer**: Infrastructure, CI/CD
- **ML Engineer**: Agent logic, model integration

---

## Phase 2: Core Infrastructure (Weeks 5-8)

### 2.1 Database Design & Implementation

**Step 1: Core Schema Design**

```sql
-- Users and Organizations
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    settings JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id),
    email VARCHAR(255) UNIQUE NOT NULL,
    role VARCHAR(50) NOT NULL,
    profile JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Conversations and Messages
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id),
    customer_id VARCHAR(255) NOT NULL,
    status VARCHAR(50) DEFAULT 'active',
    context JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID REFERENCES conversations(id),
    sender_type VARCHAR(20) NOT NULL, -- 'customer', 'agent', 'human'
    content TEXT NOT NULL,
    message_type VARCHAR(50) DEFAULT 'text',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Agent Configuration
CREATE TABLE agent_configs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id),
    name VARCHAR(255) NOT NULL,
    prompt_template TEXT NOT NULL,
    model_settings JSONB DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    version INTEGER DEFAULT 1,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Knowledge Base
CREATE TABLE knowledge_articles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id),
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    categories TEXT[],
    vector_id VARCHAR(255), -- Pinecone vector ID
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Step 2: Database Migration Strategy**
```python
# alembic/versions/001_initial_schema.py
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

def upgrade():
    # Create extensions
    op.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')
    
    # Create tables
    op.create_table('organizations',
        sa.Column('id', postgresql.UUID(), nullable=False),
        sa.Column('name', sa.String(255), nullable=False),
        # ... rest of the schema
    )
```

### 2.2 API Foundation

**Step 1: FastAPI Application Structure**
```python
# app/main.py
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from .core.config import settings
from .core.database import engine
from .api import agents, conversations, auth

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown
    await cleanup()

app = FastAPI(
    title="IntelliAgent API",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_HOSTS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(auth.router, prefix="/api/v1/auth", tags=["auth"])
app.include_router(conversations.router, prefix="/api/v1/conversations", tags=["conversations"])
app.include_router(agents.router, prefix="/api/v1/agents", tags=["agents"])
```

**Step 2: Core API Endpoints**
```python
# app/api/conversations.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional

from ..core.database import get_db
from ..schemas.conversation import ConversationCreate, ConversationResponse
from ..services.conversation_service import ConversationService

router = APIRouter()

@router.post("/", response_model=ConversationResponse)
async def create_conversation(
    conversation: ConversationCreate,
    db: AsyncSession = Depends(get_db),
    current_user = Depends(get_current_user)
):
    service = ConversationService(db)
    return await service.create_conversation(conversation, current_user.organization_id)

@router.get("/{conversation_id}/messages")
async def get_messages(
    conversation_id: str,
    db: AsyncSession = Depends(get_db),
    current_user = Depends(get_current_user)
):
    service = ConversationService(db)
    return await service.get_messages(conversation_id)

@router.post("/{conversation_id}/messages")
async def send_message(
    conversation_id: str,
    message: MessageCreate,
    db: AsyncSession = Depends(get_db),
    current_user = Depends(get_current_user)
):
    service = ConversationService(db)
    # Send to agent processing queue
    await kafka_producer.send("agent-messages", {
        "conversation_id": conversation_id,
        "message": message.dict()
    })
    return {"status": "queued"}
```

### 2.3 Authentication & Authorization

**Step 1: JWT Implementation**
```python
# app/core/auth.py
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

class AuthService:
    def __init__(self, secret_key: str, algorithm: str = "HS256"):
        self.secret_key = secret_key
        self.algorithm = algorithm
    
    def create_access_token(self, data: dict, expires_delta: Optional[timedelta] = None):
        to_encode = data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=15)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, token: str):
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            return payload
        except JWTError:
            raise HTTPException(status_code=401, detail="Invalid token")
```

### 2.4 Caching Strategy

**Step 1: Redis Integration**
```python
# app/core/cache.py
import redis.asyncio as redis
import json
from typing import Optional, Any

class CacheService:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url, decode_responses=True)
    
    async def get(self, key: str) -> Optional[Any]:
        value = await self.redis.get(key)
        if value:
            return json.loads(value)
        return None
    
    async def set(self, key: str, value: Any, expire: int = 3600):
        await self.redis.setex(key, expire, json.dumps(value, default=str))
    
    async def delete(self, key: str):
        await self.redis.delete(key)
    
    # Conversation context caching
    async def cache_conversation_context(self, conversation_id: str, context: dict):
        key = f"conversation:{conversation_id}:context"
        await self.set(key, context, expire=7200)  # 2 hours
```

---

## Phase 3: Agent Core Development (Weeks 9-14)

### 3.1 Agent Architecture Design

**Core Agent Components:**
1. **Intent Classifier**: Determines user intent from message
2. **Context Manager**: Maintains conversation state
3. **Knowledge Retriever**: Searches relevant information
4. **Response Generator**: Creates appropriate responses
5. **Action Executor**: Performs external actions (API calls, etc.)

**Step 1: Agent Base Class**
```python
# app/agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

@dataclass
class AgentMessage:
    content: str
    message_type: str
    metadata: Dict[str, Any] = None
    confidence: float = 0.0

@dataclass
class AgentContext:
    conversation_id: str
    customer_id: str
    organization_id: str
    conversation_history: List[AgentMessage]
    user_profile: Dict[str, Any]
    session_data: Dict[str, Any]

class BaseAgent(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = config.get('name', 'BaseAgent')
        self.version = config.get('version', '1.0')
    
    @abstractmethod
    async def process_message(
        self, 
        message: str, 
        context: AgentContext
    ) -> AgentMessage:
        """Process incoming message and return response"""
        pass
    
    @abstractmethod
    async def classify_intent(self, message: str) -> Dict[str, Any]:
        """Classify the intent of the message"""
        pass
    
    async def update_context(self, context: AgentContext, message: AgentMessage):
        """Update conversation context after processing"""
        context.conversation_history.append(message)
        context.session_data['last_interaction'] = datetime.utcnow()
```

### 3.2 Intent Classification System

**Step 1: Intent Classifier Implementation**
```python
# app/agents/intent_classifier.py
from transformers import pipeline
from typing import Dict, List, Any
import numpy as np

class IntentClassifier:
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.classifier = pipeline(
            "text-classification",
            model="facebook/bart-large-mnli",
            device=0 if torch.cuda.is_available() else -1
        )
        
        # Define intent categories
        self.intent_labels = [
            "billing inquiry",
            "technical support",
            "product information",
            "complaint",
            "compliment",
            "cancellation request",
            "feature request",
            "general question"
        ]
    
    async def classify(self, message: str) -> Dict[str, Any]:
        """Classify message intent with confidence scores"""
        
        # Prepare classification prompts
        sequences = [f"This message is about {label}" for label in self.intent_labels]
        
        results = []
        for sequence in sequences:
            result = self.classifier(message, sequence)
            results.append({
                'label': sequence.replace('This message is about ', ''),
                'confidence': result['scores'][result['labels'].index('ENTAILMENT')]
            })
        
        # Sort by confidence
        results.sort(key=lambda x: x['confidence'], reverse=True)
        
        return {
            'primary_intent': results[0]['label'],
            'confidence': results[0]['confidence'],
            'all_intents': results,
            'needs_escalation': results[0]['confidence'] < 0.7
        }
```

### 3.3 Knowledge Retrieval System

**Step 1: Vector Database Integration**
```python
# app/services/knowledge_service.py
import pinecone
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional

class KnowledgeService:
    def __init__(self, pinecone_api_key: str, environment: str):
        pinecone.init(api_key=pinecone_api_key, environment=environment)
        self.index = pinecone.Index("intelligagent-knowledge")
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    async def embed_text(self, text: str) -> List[float]:
        """Convert text to vector embedding"""
        return self.encoder.encode(text).tolist()
    
    async def search_knowledge(
        self, 
        query: str, 
        organization_id: str,
        top_k: int = 5,
        min_score: float = 0.7
    ) -> List[Dict[str, Any]]:
        """Search for relevant knowledge articles"""
        
        # Create query embedding
        query_vector = await self.embed_text(query)
        
        # Search in Pinecone
        results = self.index.query(
            vector=query_vector,
            filter={"organization_id": organization_id},
            top_k=top_k,
            include_metadata=True
        )
        
        # Filter by minimum score and format results
        relevant_articles = []
        for match in results['matches']:
            if match['score'] >= min_score:
                relevant_articles.append({
                    'id': match['id'],
                    'title': match['metadata']['title'],
                    'content': match['metadata']['content'],
                    'score': match['score'],
                    'categories': match['metadata'].get('categories', [])
                })
        
        return relevant_articles
    
    async def index_article(
        self, 
        article_id: str, 
        title: str, 
        content: str,
        organization_id: str,
        categories: List[str] = None
    ):
        """Add new article to knowledge base"""
        
        # Create embedding for the article
        text_to_embed = f"{title}\n\n{content}"
        vector = await self.embed_text(text_to_embed)
        
        # Upsert to Pinecone
        self.index.upsert([{
            'id': article_id,
            'values': vector,
            'metadata': {
                'title': title,
                'content': content,
                'organization_id': organization_id,
                'categories': categories or []
            }
        }])
```

### 3.4 Response Generation

**Step 1: LLM Integration with Prompt Templates**
```python
# app/agents/response_generator.py
from openai import AsyncOpenAI
from typing import Dict, Any, List, Optional
import json

class ResponseGenerator:
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
    
    async def generate_response(
        self,
        user_message: str,
        intent: Dict[str, Any],
        knowledge_articles: List[Dict[str, Any]],
        conversation_history: List[Dict[str, Any]],
        agent_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate contextual response using LLM"""
        
        # Build context from knowledge articles
        knowledge_context = ""
        if knowledge_articles:
            knowledge_context = "\n\nRelevant Information:\n"
            for article in knowledge_articles[:3]:  # Use top 3 articles
                knowledge_context += f"- {article['title']}: {article['content'][:200]}...\n"
        
        # Build conversation context
        conversation_context = ""
        if len(conversation_history) > 0:
            conversation_context = "\n\nRecent Conversation:\n"
            for msg in conversation_history[-5:]:  # Last 5 messages
                role = "Customer" if msg['sender_type'] == 'customer' else "Agent"
                conversation_context += f"{role}: {msg['content']}\n"
        
        # Create system prompt
        system_prompt = f"""
        You are {agent_config.get('name', 'a helpful customer service agent')} for {agent_config.get('company_name', 'our company')}.
        
        Your personality: {agent_config.get('personality', 'Professional, helpful, and empathetic')}
        
        Guidelines:
        - Be concise but thorough
        - Use the provided knowledge base information when relevant
        - If you cannot help, suggest escalation to a human agent
        - Always be polite and professional
        - Address the customer's specific intent: {intent['primary_intent']}
        
        {knowledge_context}
        {conversation_context}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            generated_text = response.choices[0].message.content
            
            return {
                'response': generated_text,
                'confidence': 0.9 if intent['confidence'] > 0.8 else 0.7,
                'sources': [article['id'] for article in knowledge_articles[:3]],
                'intent_handled': intent['primary_intent'],
                'needs_human': intent['needs_escalation'] or 'escalate' in generated_text.lower()
            }
            
        except Exception as e:
            return {
                'response': "I apologize, but I'm having trouble processing your request right now. Let me connect you with a human agent who can help you better.",
                'confidence': 0.0,
                'error': str(e),
                'needs_human': True
            }
```

### 3.5 Main Agent Implementation

**Step 1: Customer Support Agent**
```python
# app/agents/customer_support_agent.py
from .base_agent import BaseAgent, AgentMessage, AgentContext
from .intent_classifier import IntentClassifier
from .response_generator import ResponseGenerator
from ..services.knowledge_service import KnowledgeService
from typing import Dict, Any

class CustomerSupportAgent(BaseAgent):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.intent_classifier = IntentClassifier()
        self.response_generator = ResponseGenerator(
            api_key=config['openai_api_key'],
            model=config.get('model', 'gpt-4')
        )
        self.knowledge_service = KnowledgeService(
            config['pinecone_api_key'],
            config['pinecone_environment']
        )
    
    async def process_message(
        self, 
        message: str, 
        context: AgentContext
    ) -> AgentMessage:
        """Main processing pipeline"""
        
        try:
            # Step 1: Classify intent
            intent = await self.classify_intent(message)
            
            # Step 2: Retrieve relevant knowledge
            knowledge_articles = await self.knowledge_service.search_knowledge(
                query=message,
                organization_id=context.organization_id,
                top_k=5
            )
            
            # Step 3: Generate response
            response_data = await self.response_generator.generate_response(
                user_message=message,
                intent=intent,
                knowledge_articles=knowledge_articles,
                conversation_history=[msg.__dict__ for msg in context.conversation_history],
                agent_config=self.config
            )
            
            # Step 4: Create agent message
            agent_message = AgentMessage(
                content=response_data['response'],
                message_type='text',
                metadata={
                    'intent': intent,
                    'knowledge_sources': response_data.get('sources', []),
                    'confidence': response_data['confidence'],
                    'needs_human': response_data.get('needs_human', False)
                },
                confidence=response_data['confidence']
            )
            
            return agent_message
            
        except Exception as e:
            # Fallback response
            return AgentMessage(
                content="I apologize, but I'm experiencing technical difficulties. Let me connect you with a human agent who can assist you better.",
                message_type='text',
                metadata={
                    'error': str(e),
                    'needs_human': True,
                    'confidence': 0.0
                },
                confidence=0.0
            )
    
    async def classify_intent(self, message: str) -> Dict[str, Any]:
        """Use intent classifier"""
        return await self.intent_classifier.classify(message)
```

---

## Phase 4: Frontend Development (Weeks 15-18)

### 4.1 Next.js Application Setup

**Step 1: Project Structure**
```
frontend/
├── components/
│   ├── chat/
│   │   ├── ChatWindow.tsx
│   │   ├── MessageBubble.tsx
│   │   └── InputForm.tsx
│   ├── dashboard/
│   │   ├── ConversationList.tsx
│   │   └── AgentMetrics.tsx
│   ├── layout/
│   │   ├── Header.tsx
│   │   └── Sidebar.tsx
│   └── ui/
│       ├── Button.tsx
│       ├── Input.tsx
│       └── Modal.tsx
├── pages/
│   ├── api/
│   ├── chat/
│   ├── dashboard/
│   └── auth/
├── services/
│   ├── api.ts
│   ├── websocket.ts
│   └── auth.ts
├── hooks/
│   ├── useAuth.ts
│   ├── useChat.ts
│   └── useWebSocket.ts
├── utils/
│   ├── constants.ts
│   └── helpers.ts
└── styles/
    └── globals.css
```

**Step 2: API Service Layer**
```typescript
// services/api.ts
import axios, { AxiosInstance, AxiosRequestConfig } from 'axios';

class ApiService {
  private client: AxiosInstance;

  constructor(baseURL: string) {
    this.client = axios.create({
      baseURL,
      timeout: 10000,
    });

    // Request interceptor for auth
    this.client.interceptors.request.use((config) => {
      const token = localStorage.getItem('access_token');
      if (token) {
        config.headers.Authorization = `Bearer ${token}`;
      }
      return config;
    });

    // Response interceptor for error handling
    this.client.interceptors.response.use(
      (response) => response,
      (error) => {
        if (error.response?.status === 401) {
          // Handle token expiration
          localStorage.removeItem('access_token');
          window.location.href = '/auth/login';
        }
        return Promise.reject(error);
      }
    );
  }

  // Conversation methods
  async getConversations(page = 1, limit = 20) {
    const response = await this.client.get(`/api/v1/conversations?page=${page}&limit=${limit}`);
    return response.data;
  }

  async createConversation(customerId: string) {
    const response = await this.client.post('/api/v1/conversations', {
      customer_id: customerId
    });
    return response.data;
  }

  async sendMessage(conversationId: string, content: string) {
    const response = await this.client.post(`/api/v1/conversations/${conversationId}/messages`, {
      content,
      sender_type: 'customer'
    });
    return response.data;
  }

  async getMessages(conversationId: string) {
    const response = await this.client.get(`/api/v1/conversations/${conversationId}/messages`);
    return response.data;
  }

  // Auth methods
  async login(email: string, password: string) {
    const response = await this.client.post('/api/v1/auth/login', {
      email,
      password
    });
    return response.data;
  }

  async register(userData: any) {
    const response = await this.client.post('/api/v1/auth/register', userData);
    return response.data;
  }
}

export const apiService = new ApiService(process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000');
```

### 4.2 WebSocket Integration

**Step 1: WebSocket Service**
```typescript
// services/websocket.ts
import { io, Socket } from 'socket.io-client';

interface MessageData {
  id: string;
  conversationId: string;
  content: string;
  senderType: 'customer' | 'agent' | 'human';
  timestamp: string;
  metadata?: any;
}

interface TypingData {
  conversationId: string;
  isTyping: boolean;
}

class WebSocketService {
  private socket: Socket | null = null;
  private reconnectAttempts = 0;
  private maxReconnectAttempts = 5;

  connect(token: string): void {
    this.socket = io(process.env.NEXT_PUBLIC_WS_URL || 'ws://localhost:8000', {
      auth: {
        token
      },